{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "homeless-dover",
   "metadata": {},
   "source": [
    "## This script calculates and plots the frequencies of 2- and 3-word collocations in text. <br> Viktoria, April 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "czech-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import docx2txt\n",
    "import re\n",
    "import pdfplumber\n",
    "import textract\n",
    "import ocrmypdf\n",
    "import pluggy\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "from decimal import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import textract\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import ocrmypdf\n",
    "import pluggy\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "import pdfkit\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import random\n",
    "\n",
    "from read_pdf import read_pdf \n",
    "\n",
    "pd.options.display.max_rows = 8000\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entire-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/Users/Viktoria/Desktop/PlotCollocations'\n",
    "documents = os.path.join(base, 'Documents')\n",
    "results = os.path.join(base, 'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "former-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to the base directory to access the requirements file\n",
    "\n",
    "os.chdir(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrative-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disciplinary-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/jbarlow83/OCRmyPDF.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "molecular-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1:  10 \n",
      "Class2:  11 \n",
      "Class3:  12\n"
     ]
    }
   ],
   "source": [
    "#Get the texts from the different classes \n",
    "\n",
    "os.chdir(documents)\n",
    "\n",
    "Class1 = os.path.join(base, 'Documents/Class_1')\n",
    "Class2 = os.path.join(base, 'Documents/Class_2')\n",
    "Class3 = os.path.join(base, 'Documents/Class_3')\n",
    "\n",
    "Docs1 = [c for c in os.listdir(Class1)]\n",
    "Docs2 = [c for c in os.listdir(Class2)]\n",
    "Docs3 = [c for c in os.listdir(Class3)]\n",
    "\n",
    "\n",
    "print('Class1: ', len(Docs1), '\\nClass2: ', len(Docs2), '\\nClass3: ', len(Docs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "several-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Articificially balance the classes: take the same number at random\n",
    "# Or : tweak the ratio according to the ratio expected in the population\n",
    "\n",
    "D1 = dict.fromkeys(Docs1, 1)\n",
    "#D1 = random.choices(list(D1.items()), k = 10)\n",
    "\n",
    "D2 = dict.fromkeys(Docs2, 2)\n",
    "#D2 = random.choices(list(D2.items()), k = 10)\n",
    "\n",
    "D3 = dict.fromkeys(Docs3, 3)\n",
    "#D3 = random.choices(list(D3.items()), k = 10)\n",
    "\n",
    "#create a dataset where the keys are the document names, and the values are the class\n",
    "data = {**D1, **D2, **D3} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "athletic-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>Dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>European Parliament Regulation on Crypto-asset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/Viktoria/Desktop/PlotCollocations/Docum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FATF Report to G20 on so-called stable coins.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/Viktoria/Desktop/PlotCollocations/Docum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FSB Global Stablecoins.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/Viktoria/Desktop/PlotCollocations/Docum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G7 working group on stable coins.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/Viktoria/Desktop/PlotCollocations/Docum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JMLSG-Guidance.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/Viktoria/Desktop/PlotCollocations/Docum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Text  Class  \\\n",
       "0  European Parliament Regulation on Crypto-asset...  NaN      1   \n",
       "1   FATF Report to G20 on so-called stable coins.pdf  NaN      1   \n",
       "2                         FSB Global Stablecoins.pdf  NaN      1   \n",
       "3               G7 working group on stable coins.pdf  NaN      1   \n",
       "4                                 JMLSG-Guidance.pdf  NaN      1   \n",
       "\n",
       "                                                 Dir  \n",
       "0  /Users/Viktoria/Desktop/PlotCollocations/Docum...  \n",
       "1  /Users/Viktoria/Desktop/PlotCollocations/Docum...  \n",
       "2  /Users/Viktoria/Desktop/PlotCollocations/Docum...  \n",
       "3  /Users/Viktoria/Desktop/PlotCollocations/Docum...  \n",
       "4  /Users/Viktoria/Desktop/PlotCollocations/Docum...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loop through them all at once. 200 docs takes ~10mins\n",
    "\n",
    "df = pd.DataFrame(columns=['Document', 'Text', 'Class'])\n",
    "\n",
    "df['Document'] = data.keys()\n",
    "df['Class'] = data.values()   \n",
    "\n",
    "def get_directory(class_id):\n",
    "    \n",
    "    if class_id == 1:\n",
    "        directory = Class1\n",
    "    elif class_id == 2:\n",
    "        directory = Class2\n",
    "    elif class_id == 3:\n",
    "        directory = Class3\n",
    "        \n",
    "    return directory\n",
    "\n",
    "df['Dir'] = df['Class'].apply(get_directory)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/33 [00:17<09:34, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 2/33 [00:22<05:03,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 3/33 [00:26<03:45,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 4/33 [00:31<03:04,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 5/33 [01:12<08:45, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 6/33 [01:29<08:09, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 7/33 [01:34<05:59, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 8/33 [01:37<04:26, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 9/33 [01:42<03:32,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed pdf\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df['Text'] = [m for m in map(read_pdf, tqdm(df.Document), df.Dir)]\n",
    "      \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the silly characters from the beginning of the title until the first capital letter\n",
    "\n",
    "def replace_silly(string):\n",
    "\n",
    "    string = re.sub(r'^[^A-Z.-]+\\s*', '', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document']=df['Document'].apply(replace_silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-sustainability",
   "metadata": {},
   "source": [
    "### Text cleaning and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(content_as_words):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        #Map POS tag to first character lemmatize() accepts\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    lemma = []\n",
    "    subset = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in content_as_words]\n",
    "    #sentence = ' '.join(subset)\n",
    "    lemma.extend(subset)\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    \n",
    "    #initial text cleaning\n",
    "    if type(content) == bytes:\n",
    "        content = content.decode(\"utf-8\") \n",
    "    content = re.findall(r'[a-zA-Z]+', content)\n",
    "    content = [c.lower() for c in content]\n",
    "    \n",
    "    #lemmatize. this will return a list of 1 item: the lemmatized text as a string\n",
    "    lemma = lemmatize(content)\n",
    "\n",
    "    #get rid of non-English words\n",
    "    en_words = set(nltk.corpus.words.words())\n",
    "    stop_words = [s for s in stopwords.words('english') if s not in informative]\n",
    "    \n",
    "    #get rid of the whole thing if not in English (ratio of en_words is < 40%)\n",
    "    num_words = len(content)\n",
    "    content = [c for c in content if c in en_words or c in informative]\n",
    "    \n",
    "    if num_words == 0 or len(content)/num_words < 0.4:\n",
    "        content = ''\n",
    "    else:\n",
    "        #remove stop words\n",
    "        content = [c for c in content if c not in stop_words and c not in useless]\n",
    "        #remove short words\n",
    "        content = [c for c in content if len(c)>=3]\n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A set of words we want to screen out as corpus-specific stop words, i.e. 'article', 'paragraph', etc\n",
    "os.chdir(models)\n",
    "\n",
    "result = docx2txt.process(\"useless_words.docx\")\n",
    "useless = re.findall(r'\\w+', result)\n",
    "useless = lemmatize(useless)\n",
    "useless = [u.lower() for u in useless]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All non-English words will be disposed of. Keep important non-English words on this list.\n",
    "os.chdir(models)\n",
    "\n",
    "result = docx2txt.process(\"informative_words.docx\")\n",
    "informative = re.findall(r'\\w+', result)\n",
    "informative = [i.lower() for i in informative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through them all at once.\n",
    "\n",
    "df['Cleaned_Text'] = [m for m in map(clean_text, tqdm(df.Text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if cleaned_text is a neatly ordered list of strings (rather than one long string)\n",
    "if type(df.Cleaned_Text[0])==str:\n",
    "    df['Cleaned_Text'] = [eval(df.loc[i, 'Cleaned_Text']) for i,v in df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-paintball",
   "metadata": {},
   "source": [
    "### Calculate the number and proportion of keyphrases in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keywords\n",
    "os.chdir(coin)\n",
    "\n",
    "with open('stablecoin_keyphrases.txt', 'r+') as f:\n",
    "    keyphrases = f.readlines()  \n",
    "    keyphrases = [re.sub('\\n', '', k) for k in keyphrases]\n",
    "    keyphrases = [k.lower() for k in keyphrases]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many words do we have?\n",
    "len(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.chdir(coin)\n",
    "\n",
    "word_ls = keyphrases\n",
    "word_could_dict = Counter(word_ls)\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(word_could_dict)\n",
    "wordcloud.to_file(\"keyphrases.png\")\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = [text.decode(\"utf-8\") for text in df.Text if type(text) == bytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "for r,v in df.iterrows():\n",
    "    \n",
    "    if 'stablecoin' in df.loc[r, 'Text']:\n",
    "        print('found', r)\n",
    "        a=a+1\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[7, 'Document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[8, 'Document']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-lloyd",
   "metadata": {},
   "source": [
    "### Calculate the number of times the key phrases ocurred in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r,v in df.iterrows():\n",
    "    \n",
    "    num = 0\n",
    "\n",
    "    for k in keyphrases:\n",
    "        num = num + df.loc[r, 'Text'].lower().count(k)\n",
    "        \n",
    "    df.loc[r, 'Num_Phrases'] = num\n",
    "    df.loc[r, 'Prop_Phrases'] = num/len(df.loc[r, 'Cleaned_Text'])*100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document'] = [re.sub(r'.pdf', '', d) for d in df.Document]\n",
    "\n",
    "for r,v in df.iterrows():\n",
    "    if df.loc[r, 'Class'] == 1:\n",
    "        df.loc[r, 'Colour'] = 'red'\n",
    "    elif df.loc[r, 'Class'] == 2:\n",
    "        df.loc[r, 'Colour'] = 'blue'\n",
    "    elif df.loc[r, 'Class'] == 3:\n",
    "        df.loc[r, 'Colour'] = 'yellow'\n",
    "    elif df.loc[r, 'Class'] == 4:\n",
    "        df.loc[r, 'Colour'] = 'green'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-taiwan",
   "metadata": {},
   "source": [
    "### Plot the poroportion of documents as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[14, 'Document'] = 'Consultation on cooperation and information exchange for AML CFT supervisory purposes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[15, 'Document'] = 'Consultation on the implementation of group wide AML-CFT policies in third countries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.chdir(coin)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "#plt.bar(range(len(docs_phrases)), list(docs_phrases.values()), align='center', width=0.5, color=colours)\n",
    "#plt.xticks(range(len(docs_phrases)), list(docs_phrases.keys()), rotation=\"vertical\")\n",
    "\n",
    "plt.bar(df.Document, df.Prop_Phrases, align='center', width=0.5, color=df.Colour)\n",
    "plt.xticks(range(len(df)), df.Document.to_list(), rotation=90)\n",
    "plt.yticks(range(0,22,2))\n",
    "\n",
    "plt.title('Proportion of stablecoin-related keyphrases', fontsize = 20)\n",
    "\n",
    "red = mpatches.Patch(color='red', label='Stablecoin')\n",
    "blue = mpatches.Patch(color='blue', label='AML')\n",
    "yellow = mpatches.Patch(color='yellow', label='Crypto')\n",
    "green = mpatches.Patch(color='green', label='Payments')\n",
    "\n",
    "plt.legend(loc=7, fontsize = 'large', handles=[red, blue, yellow, green])\n",
    "\n",
    "plt.tight_layout() #to make sure the text is legible \n",
    "\n",
    "plt.savefig('Proportion of stablecoin-related keyphrases OLD.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-brush",
   "metadata": {},
   "source": [
    "### Now do some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the labelled data (from newsletters)\n",
    "os.chdir(data_repo)\n",
    "\n",
    "all_data = pd.read_csv('Classifiers_TrainingData.csv')\n",
    "all_data = all_data.loc[all_data['Cleaned_Text'].str.len()>=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure cleaned_text is an orderly list of strings\n",
    "\n",
    "if type(all_data.loc[0, 'Cleaned_Text']) == str:\n",
    "    all_data['Cleaned_Text'] = [eval(all_data.loc[i, 'Cleaned_Text']) for i,v in all_data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(all_data.columns.difference(['Document', 'Text', 'Class', 'Topic', 'Cleaned_Text']), 1, inplace=True)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants = all_data[all_data.Class==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants=relevants.reset_index(drop=True)\n",
    "relevants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants['Document'] = [re.sub(r'.pdf', '', d) for d in relevants.Document]\n",
    "\n",
    "for r,v in relevants.iterrows():\n",
    "    if relevants.loc[r, 'Topic'] == 'stablecoin':\n",
    "        relevants.loc[r, 'Colour'] = 'red'\n",
    "    elif relevants.loc[r, 'Topic'] == 'Anti_money':\n",
    "        relevants.loc[r, 'Colour'] = 'blue'\n",
    "    elif relevants.loc[r, 'Topic'] == 'Crypto':\n",
    "        relevants.loc[r, 'Colour'] = 'yellow'\n",
    "    elif relevants.loc[r, 'Topic'] == 'Payments':\n",
    "        relevants.loc[r, 'Colour'] = 'green'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Document.isin(relevants.Document).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r,v in relevants.iterrows():\n",
    "    \n",
    "    num = 0\n",
    "\n",
    "    for k in keyphrases:\n",
    "        num = num + relevants.loc[r, 'Text'].lower().count(k)\n",
    "        \n",
    "    relevants.loc[r, 'Num_Phrases'] = num\n",
    "    relevants.loc[r, 'Prop_Phrases'] = num/len(relevants.loc[r, 'Cleaned_Text'])*100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants = relevants.reset_index(drop=True)\n",
    "relevants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "stablecoin = df.groupby('Class')['Prop_Phrases'].agg(['mean', 'std'])\n",
    "stablecoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "stablecoin.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "stablecoin.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "summarise = pd.DataFrame()\n",
    "\n",
    "stats = relevants.groupby('Topic')['Prop_Phrases'].agg(['mean', 'std'])\n",
    "\n",
    "summarise['Means'] = list(stats.iloc[:,0])\n",
    "summarise['SD'] = list(stats.iloc[:,1])\n",
    "summarise['Topic'] = ['AML', 'Crypto', 'Payments']\n",
    "summarise['Colour'] = ['blue', 'yellow', 'green']\n",
    "summarise.loc[-1] = [stablecoin.iloc[0,0], stablecoin.iloc[0,1], 'Stablecoin', 'red']\n",
    "summarise.index = summarise.index+1\n",
    "summarise = summarise.sort_index()\n",
    "\n",
    "summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(coin)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for s in range(0, len(summarise.Means)):\n",
    "    summarise.Means[s] = float(Decimal(str(summarise.Means[s])).quantize(Decimal('.001'), rounding=ROUND_DOWN))\n",
    "    \n",
    "    \n",
    "plt.bar(summarise.Topic, summarise.Means, align='center', width=0.5, color=summarise.Colour, yerr=summarise.SD)\n",
    "plt.xticks(range(len(summarise)), summarise.Topic.to_list(), rotation=\"vertical\")\n",
    "\n",
    "plt.title('Average proportion of keyphrases per topic', fontsize = 20)\n",
    "\n",
    "red = mpatches.Patch(color='red', label='Stablecoin')\n",
    "blue = mpatches.Patch(color='blue', label='AML')\n",
    "yellow = mpatches.Patch(color='yellow', label='Crypto')\n",
    "green = mpatches.Patch(color='green', label='Payments')\n",
    "\n",
    "plt.legend(loc=7, fontsize = 'large', handles=[red, blue, yellow, green])\n",
    "\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(summarise.Topic)):\n",
    "            plt.text(i,y[i],y[i])\n",
    "            \n",
    "addlabels(summarise.Topic, summarise.Means)\n",
    "\n",
    "\n",
    "plt.tight_layout() #to make sure the text is legible \n",
    "\n",
    "plt.savefig('Average proportion of keyphrases per topic.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-daniel",
   "metadata": {},
   "source": [
    "### Is 2 a good threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants[relevants.Topic=='Payments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants[(relevants.Topic=='Payments') & (relevants.Prop_Phrases>2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants[relevants.Topic=='Anti_money'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants[(relevants.Topic=='Anti_money') & (relevants.Prop_Phrases>2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants[relevants.Topic=='Crypto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevants[(relevants.Topic=='Crypto') & (relevants.Prop_Phrases>2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-official",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-cigarette",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "editorial-weapon",
   "metadata": {},
   "source": [
    "### Download documents with > 2% keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the new documents\n",
    "os.chdir(models)\n",
    "\n",
    "data = pd.read_csv('Classifiers_TestData.csv')\n",
    "data.drop(data.columns.difference(['Source','Document', 'URL', 'Text', 'Cleaned_Text']), 1, inplace=True)\n",
    "\n",
    "#make sure cleaned_text is an orderly list of strings\n",
    "if type(data.loc[0, 'Cleaned_Text']) == str:\n",
    "     data['Cleaned_Text'] = [eval(data.loc[i, 'Cleaned_Text']) for i,v in data.iterrows()]\n",
    "        \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the proportion of keyphrases\n",
    "\n",
    "for r,v in data.iterrows():\n",
    "    \n",
    "    num = 0\n",
    "\n",
    "    for k in keyphrases:\n",
    "        num = num + data.loc[r, 'Text'].lower().count(k)\n",
    "        \n",
    "    data.loc[r, 'Num_Phrases'] = num\n",
    "    data.loc[r, 'Prop_Phrases'] = num/len(data.loc[r, 'Cleaned_Text'])*100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = data[data.Prop_Phrases>2]\n",
    "candidates = candidates.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the table that will be saved as the output\n",
    "\n",
    "def human_readable(text):\n",
    "    \n",
    "    if type(text) == bytes:\n",
    "        text = text.decode(\"utf-8\") \n",
    "        \n",
    "    #remove the silly characters from the beginning of the text\n",
    "    text = re.sub(r'^[^A-Z.-]+\\s*', '', text)\n",
    "    \n",
    "    text = re.sub('\\s[n]\\s', '', text) #remove the ns from the newlines\n",
    "    \n",
    "    text = re.sub('\\s[n]\\s', '', text) #remove the ns from the newlines\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z0-9.*]', ' ', text)\n",
    "    text = text[0:1000]\n",
    "        \n",
    "    #only keep meaningful characters\n",
    "   # text = re.findall(r'[A-Za-z0-9/./,]*', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(results)\n",
    "\n",
    "#Drop the machine processed text \n",
    "output = candidates.drop(['Cleaned_Text', 'Text'], axis=1) \n",
    "\n",
    "#Add human-readable text\n",
    "output['Text'] = [''.join(human_readable(t)) for t in candidates.Text] \n",
    "output['Source'] = [re.sub('.csv', '', sources) for sources in output.Source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('Stablecoin candidates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-turner",
   "metadata": {},
   "source": [
    "### Now access the URLs of the relevant documents and save them in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(title_words, response, soup, download):\n",
    "    \n",
    "    open('myfile.pdf', 'wb').write(response.content)\n",
    "                    \n",
    "    with pdfplumber.open('myfile.pdf') as pdf:\n",
    "        \n",
    "        page = pdf.pages[0]\n",
    "        text = page.extract_text()\n",
    "            \n",
    "        if text==None: #scanned pdf\n",
    "            if __name__ == '__main__':\n",
    "                ocrmypdf.ocr('myfile.pdf', 'myfile_converted.pdf', deskew=True, progress_bar = False)\n",
    "                content = textract.process('myfile_converted.pdf', method='pdfminer') #pdf\n",
    "                os.remove('myfile_converted.pdf')\n",
    "                            \n",
    "        else:\n",
    "            content = textract.process('myfile.pdf', method='pdfminer') #pdf\n",
    "            \n",
    "    if  download == 1:\n",
    "        os.rename('myfile.pdf', title_words + '.pdf')\n",
    "    else:\n",
    "        os.remove('myfile.pdf')\n",
    "            \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html(title_words, response, soup):\n",
    "    \n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "        \n",
    "    content = soup.get_text()\n",
    "    #print('processed html')\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_document(title, url):\n",
    "    \n",
    "    directory = os.path.join(results, 'Stablecoin')\n",
    "    os.chdir(directory)\n",
    "    \n",
    "    name = re.sub(r'\\W+', ' ', title)\n",
    "    name = re.sub(r'pdf', '', name)\n",
    "    name = re.sub('^\\s*', '', name)\n",
    "    name = re.sub('\\s*$', '', name)\n",
    "    name = re.sub('r[:?!]', '', name)\n",
    "    title_words = name.split()\n",
    "    title_words = ' '.join(title_words)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        #word\n",
    "        if '.docx' in url:\n",
    "            \n",
    "            docx = BytesIO(requests.get(url).content)\n",
    "            content = docx2txt.process(docx)\n",
    "            pdfkit.from_string(content, title_words + '.pdf')\n",
    "            print('processed word')\n",
    "            \n",
    "        else:\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'})\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "            #pdf\n",
    "            if '.pdf' in url or 'PDF' in soup.text[0:50]:\n",
    "                content = process_pdf(title_words, response, soup, 1)\n",
    "                print('processed pdf')\n",
    "        \n",
    "            #probably html\n",
    "            else:\n",
    "                content = process_html(title_words, response, soup)\n",
    "                pdfkit.from_url(url, title_words + '.pdf')\n",
    "                print('processed html')\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        content = 'webpage error'\n",
    "        print(title, ' ', url)\n",
    "        \n",
    "    return content\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.chdir(scraper + '/Documents_downloaded')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "    \n",
    "candidates['Text'] = [m for m in map(download_document, tqdm(candidates.Document), candidates.URL)]\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-shanghai",
   "metadata": {},
   "source": [
    "### Plot the documents to show which ones are most likely to be stablecoin-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_title(title):  \n",
    "\n",
    "    name = re.sub(r'\\W+', ' ', title)\n",
    "    name = re.sub(r'pdf', '', name)\n",
    "    name = re.sub('^\\s*', '', name)\n",
    "    name = re.sub('\\s*$', '', name)\n",
    "    title_words = name.split()\n",
    "    title_words = ' '.join(title_words)\n",
    "\n",
    "    return title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['Document'] = [m for m in map(pretty_title, candidates.Document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = candidates.sort_values('Num_Phrases', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.chdir(results)\n",
    "\n",
    "plt.figure(figsize=(30,50))\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "ax = sns.barplot(x=\"Num_Phrases\", y=\"Document\", data=candidates, palette=\"Reds_r\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "max_width = 50\n",
    "ax.set_yticklabels(textwrap.fill(y.get_text(), max_width) for y in ax.get_yticklabels())\n",
    "\n",
    "plt.title(\"Number of keyphrases in stablecoin-related documents\", size=50, pad=50)\n",
    "\n",
    "ttl = ax.title\n",
    "ttl.set_position([.5, 1.01])\n",
    "\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "def roundup(x):\n",
    "    return x if x % 100 == 0 else x + 100 - x % 100\n",
    "\n",
    "ax.set_xticks(range(0, roundup(int(max(candidates.Num_Phrases))), 100))\n",
    "ax.set_xticklabels(ax.get_xticks(), size = 20)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Stablecoin-related documents.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-minimum",
   "metadata": {},
   "source": [
    "### Check stablecoin keyphrases in segments of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(coin, 'Documents'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [t for t in os.listdir(os.path.join(coin, 'Documents')) if '.txt' in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts = pd.DataFrame(columns=['Document', 'Subtitle', 'Text'])\n",
    "\n",
    "r=0\n",
    "for t in texts:\n",
    "\n",
    "    with open(t, 'r+') as f:\n",
    "        chunk = f.read()  \n",
    "        chunk = clean_text(chunk)\n",
    "        extracts.loc[r, 'Text'] = chunk\n",
    "        extracts.loc[r, 'Subtitle'] = t\n",
    "        if 'JMLSG' in t:\n",
    "            extracts.loc[r, 'Document'] = 'JMLSG Guidance'\n",
    "        elif 'UK' in t:\n",
    "            extracts.loc[r, 'Document'] = 'UK AML Regulations 2017'\n",
    "        r=r+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the proportion of keyphrases\n",
    "\n",
    "for r,v in extracts.iterrows():\n",
    "    \n",
    "    num = 0\n",
    "\n",
    "    for k in keyphrases:\n",
    "        num = num + extracts.loc[r, 'Text'].count(k)\n",
    "        \n",
    "    extracts.loc[r, 'Num_Phrases'] = num\n",
    "    extracts.loc[r, 'Prop_Phrases'] = num/len(extracts.loc[r, 'Text'])*100\n",
    "    \n",
    "    if extracts.loc[r, 'Document'] == 'JMLSG Guidance':\n",
    "        extracts.loc[r, 'Colour'] = 'firebrick'\n",
    "    elif extracts.loc[r, 'Document'] == 'UK AML Regulations 2017':\n",
    "        extracts.loc[r, 'Colour'] = 'tomato'\n",
    "        \n",
    "    extracts.loc[r, 'Subtitle'] = re.sub(r'.txt', '', extracts.loc[r, 'Subtitle'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-purchase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.chdir(coin)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "#plt.bar(range(len(docs_phrases)), list(docs_phrases.values()), align='center', width=0.5, color=colours)\n",
    "#plt.xticks(range(len(docs_phrases)), list(docs_phrases.keys()), rotation=\"vertical\")\n",
    "\n",
    "plt.bar(extracts.Subtitle, extracts.Prop_Phrases, align='center', width=0.5, color=extracts.Colour)\n",
    "plt.xticks(range(len(extracts)), extracts.Subtitle.to_list(), rotation=90)\n",
    "\n",
    "plt.title('Stablecoin-related keyphrases in extracts', fontsize = 20)\n",
    "\n",
    "red = mpatches.Patch(color='firebrick', label='JMLSG Guidance Section 22.')\n",
    "tom = mpatches.Patch(color='tomato', label='UK AML Regulations 2017')\n",
    "\n",
    "\n",
    "plt.legend(loc=7, fontsize = 'large', handles=[red, tom])\n",
    "\n",
    "plt.tight_layout() #to make sure the text is legible \n",
    "\n",
    "plt.savefig('Stablecoin-related keyphrases in extracts.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-creator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
